{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d27369-a201-4d6d-8e2a-06af051b5e9b",
   "metadata": {},
   "source": [
    "このファイルについて\n",
    "- about\n",
    "    - 検索履歴データから館山道・関越道の各区間の5分単位の検索数を計算する\n",
    "- author: 松永\n",
    "- Input\n",
    "    - ../Input_processed_data/search_records/csv202xxx/*\n",
    "- Output\n",
    "    - 時間指定あり検索\n",
    "        - ../Input_processed_data/search_count/search-count_{tateyama or kannetsu}.csv\n",
    "    - 時間指定なし検索\n",
    "        - ../Input_processed_data/search_count/search-count_{tateyama or kannetsu}_unspecified.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f97982-b882-40ad-a32e-16cb4360f862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4671dfa-3caa-460e-9cc2-5d7d98bf923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "DATA_DIR = '../../Input_processed_data'\n",
    "\n",
    "# IC, 道路情報 csv\n",
    "IC_CSV = f'{DATA_DIR}/road_master/ic_preprocessed.csv'\n",
    "IC_NET_CSV = f'{DATA_DIR}/road_master/220303-doronet_ic.csv'\n",
    "IC_SUBNET_CSV = f'{DATA_DIR}/road_master/icnet_sub.csv'\n",
    "\n",
    "# 検索ログ csv\n",
    "SEARCH_LOG_DIR = lambda month: f'{DATA_DIR}/simple_search_records/csv{month}'\n",
    "SEARCH_LOG_CSV = lambda date: f'{SEARCH_LOG_DIR(date[:6])}/record_{date}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a068031-f055-4035-aa54-384957e7a1f4",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a3b79-97ea-4ef6-8499-46bded813fe2",
   "metadata": {},
   "source": [
    "## 道路構造データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeca744f-ae87-4eb2-aa52-4eb0b610e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モジュール内で前処理済み\n",
    "df_ic = pd.read_csv(IC_CSV, dtype={'ic_code': str})\n",
    "df_icnet = pd.read_csv(IC_NET_CSV, dtype={'start_code': str, 'end_code': str, 'road_code': str})\n",
    "sub_icnet = pd.read_csv(IC_SUBNET_CSV, dtype={'start_code': str, 'end_code': str, 'road_code': str})\n",
    "\n",
    "code2name = dict(zip(df_ic['ic_code'], df_ic['ic_name']))\n",
    "name2code = {v: k for k, v in code2name.items()}\n",
    "\n",
    "ic_graph = nx.from_pandas_edgelist(\n",
    "    df_icnet, source='start_code', target='end_code',\n",
    "    edge_attr=['distance', 'road_code', 'direction'], create_using=nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8365b8-df16-4dde-b03f-44e82db4e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区間ごとの制限速度を格納したテーブル, Map を作成\n",
    "df_limits = sub_icnet.loc[:, ['start_code', 'end_code', 'start_name', 'end_name', 'road_code', 'limit']]\n",
    "limit_dict = {\n",
    "    (s_code, e_code): lim for s_code, e_code, lim in df_limits.loc[:, ['start_code', 'end_code', 'limit']].values\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e201be5-4a51-4c38-bfd9-35d9e2ebb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_nodes_set: set = set(code2name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ab3167-73e7-4aff-9946-49c674b5f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トラカンデータが持つ区間のdataframe\n",
    "tc_segments = pd.read_pickle(f'./traffic_counter_segments.pkl')\n",
    "tc_segments_set = set(\n",
    "    [tuple(segment) for segment in tc_segments.loc[:, ['start_code', 'end_code']].values]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be239f3-92dc-41ff-b890-b98c6c3877b3",
   "metadata": {},
   "source": [
    "## 最短経路オブジェクト`route_dict`の読み込み・書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee512c0-a628-4984-9ef9-973e790b9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IC Routes...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "fname = './route_dict.pkl'\n",
    "\n",
    "if os.path.exists(fname): # 経路マップがすでに存在しているとき、それを使う\n",
    "    with open(fname, 'rb') as f:\n",
    "        print('Loading IC Routes...')\n",
    "        route_dict = pickle.load(f)\n",
    "else: # 存在していなければ計算してバイナリで保存\n",
    "    print('Calculating IC Routes...')\n",
    "    route_dict = dict(nx.all_pairs_dijkstra_path(ic_graph, weight='distance'))\n",
    "    \n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(route_dict, f)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5365fa05-d521-4bc4-b209-19ec9f0f80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4G\t./route_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "! du -h ./route_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b065b3-875f-4c08-8ebf-834c281e19f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 省略するICの集合を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a59f07-af7c-4e41-bddf-71a94dae529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('small_ic.pkl', 'rb') as f:\n",
    "    df_small_ic = pickle.load(f)\n",
    "\n",
    "small_ic_set = set(df_small_ic.ic_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309067ff-fbf5-463b-9a84-8bcddbf4c6e3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12006c6a-1b9e-4077-9b17-44ba13a94542",
   "metadata": {},
   "source": [
    "## 経路計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a0c8c9-63cd-44a4-b1d7-3e69e0d53765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_route(\n",
    "    src: str, dest: str, route_dict: Dict[str, Dict[str, List[str]]]\n",
    ") -> Optional[List[str]]:\n",
    "    if not (src in ic_nodes_set and dest in ic_nodes_set):\n",
    "        return []\n",
    "    try:\n",
    "        path = route_dict[src][dest]\n",
    "        return path\n",
    "    except: # 経路が存在しない, もしくはノードがグラフ上に存在しない場合\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc05512-3eab-47b3-8753-c67d8048ec52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_route(\n",
    "    src: str, \n",
    "    dest: str, \n",
    "    route_dict: Dict[str, Dict[str, List[str]]],\n",
    "    excluded_ic_set: Set[str] = set(),\n",
    ") -> List[str]:\n",
    "    '''\n",
    "    ic_graph上で出発地から目的地までの経路を得る関数\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    src: 出発ICコード\n",
    "    dest: 目的ICコード\n",
    "    '''\n",
    "    path = __get_route(src, dest, route_dict=route_dict)\n",
    "    \n",
    "    if len(excluded_ic_set) > 0:\n",
    "        path = [ic for ic in path if ic not in excluded_ic_set]\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb4b2eb-0d09-48c4-99d3-24744ea540b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_route_with_time(\n",
    "    src: str,\n",
    "    dest: str,\n",
    "    spec_datetime: dt.datetime,\n",
    "    spec_type: int,\n",
    "    route_dict: Dict[str, Dict[str, List[str]]],\n",
    "    excluded_ic_set: Set[str] = set(),\n",
    ") -> List[Tuple[str, dt.datetime]]:\n",
    "    '''\n",
    "    ic_graph上で出発地（src_name）から目的地（target_name）までの予想通過時刻付き経路を得る関数\n",
    "    '''\n",
    "    # 関越道・館山道 以外の道路の移動速度は80km/hと仮定する\n",
    "    DEFAULT_SPEED = 80\n",
    "    \n",
    "    path = __get_route(src, dest, route_dict=route_dict)\n",
    "\n",
    "    elapsed = dt.timedelta()\n",
    "    elapsed_time_list = [elapsed]\n",
    "    for i, (start, end) in enumerate(zip(path, path[1:])):\n",
    "        dist = ic_graph[start][end]['distance']\n",
    "        limit_speed = limit_dict.get((start, end), DEFAULT_SPEED)\n",
    "        \n",
    "        # 所要時間を積算\n",
    "        td = dt.timedelta(hours = dist / limit_speed)\n",
    "        elapsed += td\n",
    "        elapsed_time_list.append(elapsed)\n",
    "    \n",
    "    if spec_type == 1:\n",
    "        time_list = [spec_datetime + td for td in elapsed_time_list]\n",
    "    elif spec_type == 2:\n",
    "        time_list = [spec_datetime - td for td in elapsed_time_list[::-1]]\n",
    "    else:\n",
    "        time_list = [spec_datetime + td for td in elapsed_time_list]\n",
    "    \n",
    "    result = list(zip(path, time_list))\n",
    "    if len(excluded_ic_set) > 0:\n",
    "        result = [(ic, time) for (ic, time) in result if ic not in excluded_ic_set]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390b49b-27a5-4be7-a42a-d9fd29eee553",
   "metadata": {},
   "source": [
    "## 検索ログの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5f60de-dbce-4bcd-a4d5-72f3ff72b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2date(date: str, format: str = '%Y%m%d') -> dt.date:\n",
    "    return dt.datetime.strptime(date, '%Y%m%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f40d374-15bf-4521-8518-74ac414c4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log(date: str) -> cudf.DataFrame:\n",
    "    type_map = {\n",
    "        'datetime': np.datetime64,\n",
    "        'start_code': 'category',\n",
    "        'end_code': 'category',\n",
    "        'spec_datetime': np.datetime64,\n",
    "        'spec_type': 'category',\n",
    "        'car_type': 'category'\n",
    "    }\n",
    "    if not os.path.exists(SEARCH_LOG_CSV(date)):\n",
    "        return cudf.DataFrame(columns=type_map.keys())\n",
    "\n",
    "    df = pd.read_csv(SEARCH_LOG_CSV(date)).astype(type_map)\n",
    "    df = df.loc[\n",
    "        (df.start_code.isin(ic_nodes_set)) &\n",
    "        (df.end_code.isin(ic_nodes_set))\n",
    "    ]\n",
    "    return cudf.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "777466f0-06b7-4454-9dd6-1174b0778ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_logs(date: str, periods: int, include_target: bool = False) -> cudf.DataFrame:\n",
    "    '''\n",
    "    指定日(target_date)から過去数日分の検索履歴データを取得する関数\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    date: 混雑度算出の対象となる日付（文字列 or リスト)\n",
    "    periods: 過去何日分の履歴を参照するか\n",
    "    include_target: dateをデータに含めるかどうか\n",
    "    '''\n",
    "    # 参照すべき全日付のiterableを生成\n",
    "    if include_target:\n",
    "        dt_range = pd.date_range(end=date, periods=periods)\n",
    "    else:\n",
    "        dt_range = pd.date_range(end=date, periods=periods+1, closed='left')\n",
    "\n",
    "    DAYS = [d.strftime('%Y%m%d') for d in dt_range]\n",
    "\n",
    "    df_specified = cudf.DataFrame()\n",
    "    for d in DAYS:\n",
    "        df = get_log(d)\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        series_spec_date: pd.Series = df.spec_datetime.to_pandas().map(lambda dt: dt.date())\n",
    "        _df_specified = df.loc[series_spec_date == str2date(date)]\n",
    "        df_specified = cudf.concat([df_specified, _df_specified], ignore_index=True)\n",
    "    df_specified.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if len(df_specified) == 0:\n",
    "        df_specified = cudf.DataFrame(columns=df.columns)\n",
    "    return df_specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18031349-079e-401b-a9a4-dae14a76d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unspecified_log(date: str) -> cudf.DataFrame:\n",
    "    '''\n",
    "    検索履歴全体から時間指定なしの検索のみを得る関数\n",
    "    '''\n",
    "    ALLOWED_MINUTE_LAG = 15\n",
    "    \n",
    "    df = get_log(date)\n",
    "    # 検索日時と指定日時の差を秒単位で算出\n",
    "    t_diff = abs((df.datetime - df.spec_datetime) / np.timedelta64(1, 's'))\n",
    "    \n",
    "    # 事前に定義した時間差（=15分）より検索日時と指定日時の差が少ないレコードのみを抽出\n",
    "    df_unspecified = df.loc[t_diff < ALLOWED_MINUTE_LAG * 60].reset_index(drop=True)\n",
    "    return df_unspecified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5003f45-27fc-4455-abc4-85ea863ae02b",
   "metadata": {},
   "source": [
    "## 検索ログのマッピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f24db9e-5edb-4808-aadb-54262ee78ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_search_to_road_network(\n",
    "    df: cudf.DataFrame,\n",
    "    route_dict: Dict[str, Dict[str, List[str]]],\n",
    "    excluded_ic_set: Set[str] = set()\n",
    ") -> cudf.DataFrame:\n",
    "    '''\n",
    "    検索ログから各道路区間の予想通過時刻を計算する\n",
    "    '''\n",
    "    start_code_list = []\n",
    "    end_code_list = []\n",
    "    passing_time_list = []\n",
    "    \n",
    "    columns = ['start_code', 'end_code', 'spec_datetime', 'spec_type']\n",
    "    for (src, dest, spec_datetime, spec_type) in df.loc[:, columns].to_pandas().values:\n",
    "        path = get_route_with_time(\n",
    "            src, dest, spec_datetime, spec_type, \n",
    "            route_dict=route_dict, \n",
    "            excluded_ic_set=excluded_ic_set\n",
    "        )\n",
    "        for i, (start, end) in enumerate(zip(path, path[1:])):\n",
    "            s_code, e_code = start[0], end[0]\n",
    "            s_ptime = start[-1]\n",
    "            \n",
    "            # if ic_graph[s_code][e_code]['road_code'] not in target_road_code_set:\n",
    "            #     break\n",
    "            \n",
    "            start_code_list.append(s_code)\n",
    "            end_code_list.append(e_code)\n",
    "            passing_time_list.append(s_ptime)\n",
    "            \n",
    "        if spec_type not in {1, 2} and len(path) > 0:\n",
    "            with open('./error_spec_type_records.csv', 'a') as f:\n",
    "                print(f'{s_ptime},{src},{dest},{spec_datetime},{spec_type}', file=f)\n",
    "                \n",
    "\n",
    "\n",
    "    return cudf.DataFrame({\n",
    "        'start_code': start_code_list,\n",
    "        'end_code': end_code_list,\n",
    "        'passing_time': passing_time_list,\n",
    "    }).astype({\n",
    "        'start_code': 'category',\n",
    "        'end_code': 'category'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b8461c-7071-4cce-bb84-f6f5d9771c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_search_count(df: cudf.DataFrame, timeslice: str = '5min') -> cudf.DataFrame:\n",
    "    # 検索量として積算される対象となるカラムを付与\n",
    "    result = df.assign(search=1)\n",
    "    \n",
    "    # timesliceでサンプリングし、検索量の和を取る\n",
    "    result = (result\n",
    "              .set_index('passing_time')\n",
    "              .to_pandas()\n",
    "              .groupby(['start_code', 'end_code'])\n",
    "              .apply(lambda g: g['search'].resample(timeslice).sum())\n",
    "              .reset_index())\n",
    "    return cudf.from_pandas(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845e445-127c-41c7-99dd-766e8ade3f73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 検索数の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4fc9-4a55-4369-81e6-72ba8eda5e37",
   "metadata": {},
   "source": [
    "## 時間指定あり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52d52036-e950-4f9a-aacf-96e018dd8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_blocks = [\n",
    "    # ('20210402', '20210630'),\n",
    "    # ('20210701', '20210930'),\n",
    "    # ('20211001', '20211231'),\n",
    "    # ('20220101', '20220331'),\n",
    "    # ('20220401', '20220630'),\n",
    "    # ('20220701', '20220930'),\n",
    "    ('20221001', '20221231'),\n",
    "    ('20230101', '20230331'),\n",
    "    ('20230401', '20230630'),\n",
    "    ('20230701', '20230731'),\n",
    "    ('20230801', '20230816'),\n",
    "    ('20230817', '20230930'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2338e9fe-3e0c-439f-985c-72f009e15156",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_periods = 7\n",
    "timeslice = '5min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6371c-f389-4c18-85bd-9c65ea95c9d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== 20221001 -> 20221231 ========================================\n",
      "20221001 | 23040 records (49.901 sec)\n",
      "20221002 | 23039 records (32.963 sec)\n",
      "20221003 | 23034 records (24.963 sec)\n",
      "20221004 | 23029 records (15.668 sec)\n",
      "20221005 | 23031 records (20.169 sec)\n",
      "20221006 | 23040 records (21.342 sec)\n",
      "20221007 | 23040 records (34.183 sec)\n",
      "20221008 | 23040 records (65.459 sec)\n",
      "20221009 | 23038 records (47.246 sec)\n",
      "20221010 | 23039 records (24.435 sec)\n",
      "20221011 | 23022 records (28.638 sec)\n",
      "20221012 | 23031 records (24.290 sec)\n",
      "20221013 | 23040 records (23.523 sec)\n",
      "20221014 | 23040 records (31.208 sec)\n",
      "20221015 | 23040 records (49.438 sec)\n",
      "20221016 | 23001 records (34.991 sec)\n",
      "20221017 | 23032 records (26.069 sec)\n",
      "20221018 | 23040 records (22.782 sec)\n",
      "20221019 | 23026 records (17.836 sec)\n",
      "20221020 | 22951 records (25.446 sec)\n",
      "20221021 | 23040 records (31.437 sec)\n",
      "20221022 | 23040 records (59.247 sec)\n",
      "20221023 | 23040 records (30.093 sec)\n",
      "20221024 | 23040 records (25.657 sec)\n",
      "20221025 | 22992 records (22.319 sec)\n",
      "20221026 | 23027 records (22.641 sec)\n",
      "20221027 | 23040 records (18.209 sec)\n",
      "20221028 | 23038 records (32.048 sec)\n",
      "20221029 | 23040 records (59.191 sec)\n",
      "20221030 | 23039 records (37.157 sec)\n",
      "20221031 | 23006 records (27.182 sec)\n",
      "20221101 | 23039 records (21.785 sec)\n",
      "20221102 | 23040 records (27.290 sec)\n",
      "20221103 | 23040 records (49.582 sec)\n",
      "20221104 | 23040 records (34.984 sec)\n",
      "20221105 | 23040 records (54.611 sec)\n",
      "20221106 | 23035 records (34.812 sec)\n",
      "20221107 | 22945 records (21.292 sec)\n",
      "20221108 | 23039 records (22.879 sec)\n",
      "20221109 | 22999 records (21.819 sec)\n",
      "20221110 | 23040 records (22.765 sec)\n",
      "20221111 | 23035 records (23.485 sec)\n",
      "20221112 | 23035 records (54.441 sec)\n",
      "20221113 | 22964 records (31.656 sec)\n",
      "20221114 | 23000 records (19.966 sec)\n",
      "20221115 | 22936 records (21.574 sec)\n",
      "20221116 | 22736 records (20.014 sec)\n",
      "20221117 | 23019 records (14.097 sec)\n",
      "20221118 | 23040 records (27.254 sec)\n",
      "20221119 | 23040 records (42.024 sec)\n",
      "20221120 | 23033 records (28.521 sec)\n",
      "20221121 | 23039 records (21.296 sec)\n",
      "20221122 | 23038 records (21.238 sec)\n",
      "20221123 | 23040 records (20.793 sec)\n",
      "20221124 | 22952 records (22.103 sec)\n",
      "20221125 | 23040 records (25.389 sec)\n",
      "20221126 | 23040 records (40.203 sec)\n",
      "20221127 | 22979 records (28.429 sec)\n",
      "20221128 | 23040 records (14.676 sec)\n",
      "20221129 | 22829 records (17.371 sec)\n",
      "20221130 | 23038 records (16.985 sec)\n",
      "20221201 | 23028 records (10.515 sec)\n",
      "20221202 | 23024 records (19.145 sec)\n",
      "20221203 | 23039 records (25.857 sec)\n",
      "20221204 | 23040 records (22.204 sec)\n",
      "20221205 | 22849 records (18.028 sec)\n",
      "20221206 | 23021 records (10.174 sec)\n",
      "20221207 | 22324 records (16.104 sec)\n",
      "20221208 | 23018 records (9.688 sec)\n",
      "20221209 | 23040 records (19.527 sec)\n",
      "20221210 | 23040 records (25.725 sec)\n",
      "20221211 | 22970 records (21.475 sec)\n",
      "20221212 | 23007 records (11.577 sec)\n",
      "20221213 | 22688 records (15.197 sec)\n",
      "20221214 | 22916 records (8.912 sec)\n",
      "20221215 | 22834 records (15.418 sec)\n",
      "20221216 | 23019 records (14.157 sec)\n",
      "20221217 | 23036 records (30.053 sec)\n",
      "20221218 | 22958 records (22.235 sec)\n",
      "20221219 | 22915 records (12.029 sec)\n",
      "20221220 | 23031 records (15.796 sec)\n",
      "20221221 | 22650 records (9.680 sec)\n",
      "20221222 | 23004 records (16.016 sec)\n",
      "20221223 | 23040 records (14.250 sec)\n",
      "20230813 | 23040 records (71.378 sec)\n",
      "20230814 | 23040 records (60.688 sec)\n",
      "20230815 | 23040 records (46.879 sec)\n",
      "20230816 | 23040 records (52.966 sec)\n",
      "======================================== 20230817 -> 20230930 ========================================\n",
      "20230817 | 23040 records (44.326 sec)\n",
      "20230818 | 23040 records (49.559 sec)\n",
      "20230819 | 23040 records (66.849 sec)\n",
      "20230820 | 23040 records (43.378 sec)\n",
      "20230821 | 23040 records (34.350 sec)\n"
     ]
    }
   ],
   "source": [
    "for start_date, end_date in period_blocks:\n",
    "    print('='*40, f'{start_date} -> {end_date}', '='*40)\n",
    "    \n",
    "    DAYS: List[str] = [\n",
    "        d.strftime('%Y%m%d') for d in pd.date_range(start_date, end_date, freq='1D')\n",
    "    ]\n",
    "    \n",
    "    df_search = cudf.DataFrame()\n",
    "    for i, date in enumerate(DAYS):\n",
    "        s = time.time()\n",
    "        \n",
    "        df = get_past_logs(date, periods=past_periods)\n",
    "        \n",
    "        df_mapped = map_search_to_road_network(\n",
    "            df, route_dict, excluded_ic_set=small_ic_set\n",
    "        )\n",
    "        df_mapped = df_mapped.loc[\n",
    "            df_mapped.loc[:, ['start_code', 'end_code']].to_pandas().apply(\n",
    "                lambda segment: tuple(segment) in tc_segments_set, axis=1\n",
    "            )\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        _df_search = aggregate_search_count(df_mapped, timeslice)\n",
    "        _df_search = cudf.from_pandas(\n",
    "            _df_search.set_index('passing_time').to_pandas().loc[date]\n",
    "        ).reset_index()\n",
    "\n",
    "        print(f'{date} | {len(_df_search)} records ({time.time() - s:.3f} sec)')\n",
    "\n",
    "        df_search = cudf.concat([df_search, _df_search], ignore_index=True)\n",
    "\n",
    "    # df_count.reset_index(inplace=True)\n",
    "    OUTPUT_FILE = f'./search_count/search-count_specified_{start_date}-{end_date}.csv'\n",
    "    df_search.to_pandas().to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38fb2bf-c895-4188-923e-8fe0e7a4b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passing_time,start_code,end_code,search\n",
      "2023-08-17 00:00:00,1040013,1040016,11\n",
      "2023-08-17 00:05:00,1040013,1040016,4\n",
      "2023-08-17 00:10:00,1040013,1040016,6\n",
      "2023-08-17 00:15:00,1040013,1040016,16\n"
     ]
    }
   ],
   "source": [
    "!head -n5 \"$OUTPUT_FILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b30094cb-c478-4dc1-8b80-f367ee8b71c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-30 23:35:00,1800106,1800111,2\n",
      "2023-09-30 23:40:00,1800106,1800111,0\n",
      "2023-09-30 23:45:00,1800106,1800111,0\n",
      "2023-09-30 23:50:00,1800106,1800111,0\n",
      "2023-09-30 23:55:00,1800106,1800111,4\n"
     ]
    }
   ],
   "source": [
    "!tail -n5 \"$OUTPUT_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b964ee9-4a8a-472a-b6a5-4c1291d6d544",
   "metadata": {},
   "source": [
    "## 時間指定なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d9477c9a-959f-4321-a52b-4db038d5f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "period_blocks = [\n",
    "    ('20210401', '20210630'),\n",
    "    ('20210701', '20210930'),\n",
    "    ('20211001', '20211231'),\n",
    "    ('20220101', '20220331'),\n",
    "    ('20220401', '20220630'),\n",
    "    ('20220701', '20220930'),\n",
    "    ('20221001', '20221231'),\n",
    "    ('20230101', '20230331'),\n",
    "    ('20230401', '20230630'),\n",
    "    ('20230701', '20230731'),\n",
    "    ('20230801', '20230816'),\n",
    "    ('20230817', '20230930'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc5a634e-f0ea-4c6d-8cd6-67d7d1768fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_search_count(df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    # 検索量として積算される対象となるカラムを付与\n",
    "    result = df.assign(search=1)\n",
    "    \n",
    "    # timesliceでサンプリングし、検索量の和を取る\n",
    "    result = (result\n",
    "              .set_index('passing_time')\n",
    "              .to_pandas()\n",
    "              .groupby(['start_code', 'end_code'])\n",
    "              .apply(lambda g: g['search'].sum())\n",
    "              .reset_index()\n",
    "              .rename(columns={0: 'search'}))\n",
    "    return cudf.from_pandas(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f48c6b-8dc9-4bd5-bc21-77fd3a2f07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== 20210401 -> 20210630 ========================================\n",
      "20210401 [51.539 sec]\n",
      "20210402 [52.568 sec]\n",
      "20210403 [49.076 sec]\n",
      "20210404 [73.560 sec]\n",
      "20210405 [84.778 sec]\n",
      "20210406 [78.978 sec]\n",
      "20210407 [82.405 sec]\n",
      "20210408 [25.712 sec]\n",
      "20210409 [32.248 sec]\n",
      "20210410 [31.325 sec]\n",
      "20210411 [25.358 sec]\n",
      "20210412 [33.609 sec]\n",
      "20210413 [33.417 sec]\n",
      "20210414 [25.201 sec]\n",
      "20210415 [32.002 sec]\n",
      "20210416 [33.906 sec]\n",
      "20210417 [41.943 sec]\n",
      "20210418 [44.776 sec]\n",
      "20210419 [44.039 sec]\n",
      "20210420 [34.789 sec]\n",
      "20210421 [39.254 sec]\n",
      "20210422 [41.728 sec]\n",
      "20210423 [42.148 sec]\n",
      "20210424 [64.635 sec]\n",
      "20210425 [51.375 sec]\n",
      "20210426 [50.683 sec]\n",
      "20210427 [50.055 sec]\n",
      "20210428 [47.648 sec]\n",
      "20210429 [46.799 sec]\n",
      "20210430 [49.963 sec]\n",
      "20210501 [52.471 sec]\n",
      "20210502 [48.047 sec]\n",
      "20210503 [36.284 sec]\n",
      "20210504 [37.087 sec]\n",
      "20210505 [39.723 sec]\n",
      "20210506 [33.031 sec]\n",
      "20210507 [28.769 sec]\n",
      "20210508 [19.068 sec]\n",
      "20210509 [24.110 sec]\n",
      "20210510 [19.783 sec]\n",
      "20210511 [25.542 sec]\n",
      "20210512 [18.797 sec]\n",
      "20210513 [26.133 sec]\n",
      "20210514 [19.506 sec]\n",
      "20210515 [24.920 sec]\n",
      "20210516 [16.701 sec]\n",
      "20210517 [19.952 sec]\n",
      "20210518 [25.431 sec]\n",
      "20210519 [20.839 sec]\n",
      "20210520 [23.595 sec]\n",
      "20210521 [28.138 sec]\n",
      "20210522 [21.591 sec]\n",
      "20210523 [18.474 sec]\n",
      "20210524 [28.495 sec]\n",
      "20210525 [29.047 sec]\n"
     ]
    }
   ],
   "source": [
    "for start_date, end_date in period_blocks:\n",
    "    print('='*40, f'{start_date} -> {end_date}', '='*40)\n",
    "    \n",
    "    DAYS: List[str] = [\n",
    "        d.strftime('%Y%m%d') for d in pd.date_range(start_date, end_date, freq='1D')\n",
    "    ]\n",
    "    \n",
    "    df_search = cudf.DataFrame()\n",
    "    for i, date in enumerate(DAYS):\n",
    "        s = time.time()\n",
    "        \n",
    "        df = get_unspecified_log(date)\n",
    "        \n",
    "        df_mapped = map_search_to_road_network(\n",
    "            df, route_dict, excluded_ic_set=small_ic_set\n",
    "        )\n",
    "        df_mapped = df_mapped.loc[\n",
    "            df_mapped.loc[:, ['start_code', 'end_code']].to_pandas().apply(\n",
    "                lambda segment: tuple(segment) in tc_segments_set, axis=1\n",
    "            )\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        _df_search = aggregate_search_count(df_mapped)\n",
    "        _df_search = (_df_search\n",
    "                      .assign(search_date=str2date(date).strftime('%Y-%m-%d'))\n",
    "                      .loc[:, ['search_date', 'start_code', 'end_code', 'search']])\n",
    "\n",
    "        print(f'{date} [{time.time() - s:.3f} sec]')\n",
    "\n",
    "        df_search = cudf.concat([df_search, _df_search], ignore_index=True)\n",
    "\n",
    "    OUTPUT_FILE = f'./search_count/search-count_unspecified_{start_date}-{end_date}.csv'\n",
    "    df_search.to_pandas().to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88ac69a1-db23-4915-82f2-7ed9757cda3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_date,start_code,end_code,search\n",
      "2023-08-17,1040013,1040016,5363\n",
      "2023-08-17,1040016,1040013,4760\n",
      "2023-08-17,1040016,1040020,6152\n",
      "2023-08-17,1040020,1040016,4521\n"
     ]
    }
   ],
   "source": [
    "!head -n5 \"$OUTPUT_FILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d80e39a-4760-4aec-a47b-76a2f1a7cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-30,1800091,1800096,4219\n",
      "2023-09-30,1800096,1800091,3744\n",
      "2023-09-30,1800096,1800106,4169\n",
      "2023-09-30,1800106,1800096,3312\n",
      "2023-09-30,1800106,1800111,3918\n"
     ]
    }
   ],
   "source": [
    "!tail -n5 \"$OUTPUT_FILE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd058145-f2fa-45db-9ebb-bcd600db2f40",
   "metadata": {},
   "source": [
    "### 時間指定なし検索を1日後ろにシフトする\n",
    "日付Dのレコードに対して、昨日(D-1)の時間指定なし検索数を示すようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a8ee6f-39e2-43b8-abe2-9baa22e0e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_unspecified_search(df):\n",
    "    res = (df\n",
    "           .groupby(['start_code', 'end_code'])\n",
    "           .apply(lambda g: g.search.shift(1))\n",
    "           # 関越道の場合はshift後にpivot_tableっぽくなってしまう（通常はpd.Series）ため、特別に処理\n",
    "           .pipe(lambda _df: _df.stack().rename('search') if not isinstance(_df, pd.Series) else _df)\n",
    "           .reset_index()\n",
    "           .sort_values(['search_date', 'start_code', 'end_code'])\n",
    "           .reset_index(drop=True)\n",
    "           .loc[:, ['search_date', 'start_code', 'end_code', 'search']])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c25c123-ea90-4751-bf6b-2b0e84bce55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unspecified = shift_unspecified_search(df_unspecified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
